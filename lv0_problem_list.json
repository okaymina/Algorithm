import requests
from bs4 import BeautifulSoup
import json
import time

def crawl_programmers_lv0_problems(pages=7):
    base_url = "https://school.programmers.co.kr"
    problems = []

    for page in range(1, pages + 1):
        url = f"{base_url}/learn/challenges/training?order=acceptance_desc&page={page}"
        headers = {
            "User-Agent": "Mozilla/5.0"
        }
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, "html.parser")

        cards = soup.select("div.css-1n6g4vv a")  # 최신 구조에 맞는 selector

        for card in cards:
            title_tag = card.select_one("span")
            if not title_tag:
                continue
            title = title_tag.get_text(strip=True)
            link = base_url + card.get("href")
            problems.append({"title": title, "url": link})

        time.sleep(0.5)

    return problems

# 크롤링 후 JSON 저장
problems = crawl_programmers_lv0_problems(pages=7)  # 약 120개
with open("lv0_problem_list.json", "w", encoding="utf-8") as f:
    json.dump(problems, f, ensure_ascii=False, indent=2)

print(f"✅ 총 {len(problems)}개 문제 저장 완료!")
